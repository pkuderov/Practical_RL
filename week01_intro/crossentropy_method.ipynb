{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crossentropy method\n",
    "\n",
    "This notebook will teach you to solve reinforcement learning problems with crossentropy method. We'll follow-up by scaling everything up and using neural network policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Google Colab, uncomment this:\n",
    "# !wget https://bit.ly/2FMJP5K -O setup.py && bash setup.py\n",
    "\n",
    "# XVFB will be launched if you run on a server\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY = : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "env = gym.make(\"Taxi-v2\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_states=500, n_actions=6\n",
      "312\n",
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# state_space = ((row * 5 + col) * 5 + pickup) * 4 + drop\n",
    "# action_space = {'down': 0, 'up': 1, 'right': 2, 'left': 3, 'pickup': 4, 'drop': 5}\n",
    "\n",
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"n_states=%i, n_actions=%i\" % (n_states, n_actions))\n",
    "print(env.reset())\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create stochastic policy\n",
    "\n",
    "This time our policy should be a probability distribution.\n",
    "\n",
    "```policy[s,a] = P(take action a | in state s)```\n",
    "\n",
    "Since we still use integer state and action representations, you can use a 2-dimensional array to represent the policy.\n",
    "\n",
    "Please initialize policy __uniformly__, that is, probabililities of all actions should be equal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.full((n_states, n_actions), 1./n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(policy) in (np.ndarray, np.matrix)\n",
    "assert np.allclose(policy, 1./n_actions)\n",
    "assert np.allclose(np.sum(policy, axis=1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play the game\n",
    "\n",
    "Just like before, but we also record all states and actions we took."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(policy, t_max=10**4):\n",
    "    \"\"\"\n",
    "    Play game until end or for t_max ticks.\n",
    "    :param policy: an array of shape [n_states,n_actions] with action probabilities\n",
    "    :returns: list of states, list of actions and sum of rewards\n",
    "    \"\"\"\n",
    "    states, actions = [], []\n",
    "    total_reward = 0.\n",
    "\n",
    "    s = env.reset()\n",
    "    for t in range(t_max):\n",
    "        a = np.random.choice(n_actions, p=policy[s])\n",
    "        new_s, r, done, info = env.step(a)\n",
    "\n",
    "        # Record state, action and add up reward to states,actions and total_reward accordingly.\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward += r\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "    return states, actions, total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, a, r = generate_session(policy)\n",
    "assert type(s) == type(a) == list\n",
    "assert len(s) == len(a)\n",
    "assert type(r) in [float, np.float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f6a65bcee80>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAVtElEQVR4nO3dfZQV9Z3n8fd3BMXHFRVctGPAOZiACKgtQmQIGwSJJiKJGomZkIQEnYyZTGYnI+o5mgfnBFc3D57kZI5PC0lcY0SjjnF3QVfGaDb2QEYTA0kalZhWBEQzoxE1hu/+cYu2hcZ+uN1098/365w+t+p3q259u7h8uu6vqn43MhNJUln+rK8LkCT1PMNdkgpkuEtSgQx3SSqQ4S5JBRrU1wUAHHLIITly5Mi+LkOSBpTVq1c/m5nD2nuuX4T7yJEjWbVqVV+XIUkDSkT8dlfP2S0jSQUy3CWpQIa7JBWoX/S5S+odf/zjH2lpaeHll1/u61JUhyFDhtDQ0MDgwYM7vY7hLhWspaWF/fffn5EjRxIRfV2OuiEz2bJlCy0tLYwaNarT69ktIxXs5Zdf5uCDDzbYB7CI4OCDD+7yp68Owz0iboiITRHxaJu2gyJiRUQ0V49Dq/aIiKsjYl1E/DwijuvybyKpRxnsA193/g07c+S+BJi9Q9si4N7MHA3cW80DvBcYXf0sBL7d5YokSXXrMNwz837guR2a5wBLq+mlwBlt2r+TNT8FDoyIET1VrKSBZ+TIkRxzzDFMnDiRxsbG1vbnnnuOmTNnMnr0aGbOnMnzzz8PwJIlS/jCF74AwO23386aNWta15k+ffqAuuFxyZIlPP30063zn/zkJ1t/n5EjR/Lss8/22ra72+d+aGZuAKgeh1fthwO/a7NcS9W2k4hYGBGrImLV5s2bu1mG6jF9yXSmL5ne12XsbPr02o+Kcd999/Hwww+/IZgXL17MjBkzaG5uZsaMGSxevHin9XYM993hT3/6U4+91o7hft111zF27Ngee/0309MnVNvrGGr3q54y85rMbMzMxmHD2h0aQVLB7rjjDubPnw/A/Pnzuf322wHYe++92W+//fjJT37CnXfeyec//3kmTpzIY489BsAtt9zCpEmTOOqoo/jxj3+80+uuXLmSadOmMXfuXMaOHcv555/Ptm3bAFi+fDlTpkzhuOOO46yzzuLFF18EakfRX/rSl5g6dSq33HIL69at4+STT2bChAkcd9xxrdu+8sorOeGEExg/fjyXXXYZAOvXr2fMmDF86lOf4uijj2bWrFls3bqVZcuWsWrVKs4991wmTpzI1q1bd/nJ43vf+x6TJk1i4sSJnHfeeT3yB6a7l0JujIgRmbmh6nbZVLW3AG9rs1wD8PROa0vqEz39SW3lx1Z2uExEMGvWLCKC8847j4ULFwKwceNGRoyo9dqOGDGCTZtqMfKhD32odd3TTz+d973vfZx55pmtba+99hpNTU3cfffdfPGLX+See+7ZaZtNTU2sWbOGt7/97cyePZvbbruN6dOnc/nll3PPPfew7777csUVV/DVr36VSy+9FKhdS/7AAw8AcOKJJ7Jo0SLmzp3Lyy+/zLZt21i+fDnNzc00NTWRmZx++uncf//9HHHEETQ3N3PTTTdx7bXXcvbZZ3PrrbfykY98hG9+85tcddVVb+iO2tHatWu5+eabefDBBxk8eDCf/vSnufHGG/noRz/a4b59M90N9zuB+cDi6vGONu0XRMT3gROBf9/efSPprenBBx/ksMMOY9OmTcycOZN3vvOdTJs2rduv94EPfACA448/nvXr17e7zKRJkzjyyCMBmDdvHg888ABDhgxhzZo1nHTSSQC8+uqrTJkypXWd7X9UXnjhBZ566inmzp0L1EIfakf9y5cv59hjjwXgxRdfpLm5mSOOOIJRo0YxceLEDutqz7333svq1as54YQTANi6dSvDhw/vYK2OdRjuEXETMB04JCJagMuohfoPImIB8CRwVrX43cCpwDrgJeDjdVcoqcd05ki7px122GEADB8+nLlz59LU1MS0adM49NBD2bBhAyNGjGDDhg2dDrS99toLgD322IPXXnut3WV2vHQwIshMZs6cyU033dTuOvvuuy9Qu2moPZnJRRddxHnnnfeG9vXr17fWtL2urVu3dup32f668+fP5ytf+Uqn1+mMzlwtMy8zR2Tm4MxsyMzrM3NLZs7IzNHV43PVspmZf52Zf56Zx2TmwDmtLanH/eEPf+CFF15onV6+fDnjxo0Dal0uS5fWLrpbunQpc+bM2Wn9/fffv3X9rmhqauKJJ55g27Zt3HzzzUydOpXJkyfz4IMPsm7dOgBeeuklfvOb3+y07gEHHEBDQ0PrOYBXXnmFl156iVNOOYUbbrihtZ/+qaeeau1K2pXO1D9jxgyWLVvW+lrPPfccv/3tLkfy7TTvUJXUazZu3MjUqVOZMGECkyZN4rTTTmP27NptM4sWLWLFihWMHj2aFStWsGjRop3WP+ecc7jyyis59thjW09qdsaUKVNYtGgR48aNY9SoUcydO5dhw4axZMkS5s2bx/jx45k8eTK/+tWv2l3/u9/9LldffTXjx4/nXe96F8888wyzZs3iwx/+MFOmTOGYY47hzDPP7DC4P/axj3H++ee3nlBtz9ixY7n88suZNWsW48ePZ+bMmWzYUH9vduzqI8ju1NjYmAPp2tVSbD+51hcf1d/U9ssgV67syyqKsHbtWsaMGdPXZexWK1eu5KqrruKuu+7q61J6VHv/lhGxOjPbPVvrkbskFchRISUVZfr06Uz3JjiP3CWpRIa7JBXIcJekAhnuklQgw11Sr/rGN77BuHHjOProo/n617/e2u6Qv/1zyF9J6tCjjz7KtddeS1NTE4888gh33XUXzc3NgEP+9jbDXVKvWbt2LZMnT2afffZh0KBBvPvd7+aHP/wh4JC/bfWnIX8lDUQ9ff13B3cRjxs3jksuuYQtW7aw9957c/fdd7cOf+uQvzX9bchfSerQmDFjuPDCC5k5cyb77bcfEyZMYNCg+mLHIX87x3CX3kr6YLyeBQsWsGDBAgAuvvhiGhoaABzyt83r9smQv5JUj+3dLU8++SS33XYb8+bNAxzydzuH/JU0IH3wgx9k7NixvP/97+db3/oWQ4cOBRzydzuH/FWPc8jf8jnkbzkc8leS5AlVSWVxyN8aj9ylwvWHrlfVpzv/hoa7VLAhQ4awZcsWA34Ay0y2bNnSer19Z9ktIxWsoaGBlpYWNm/e3NelqA5DhgxpvT+gswx3qWCDBw9m1KhRfV2G+oDdMpJUIMNdkgpkuEtSgQx3SSqQ4S5JBTLcJalAhrskFchwl6QCGe6SVKC6wj0iPhcRv4yIRyPipogYEhGjIuKhiGiOiJsjYs+eKlaS1DndDveIOBz4G6AxM8cBewDnAFcAX8vM0cDzwIKeKFSS1Hn1dssMAvaOiEHAPsAG4D3Asur5pcAZdW5DktRF3Q73zHwKuAp4klqo/zuwGvh9Zm7/SvIW4PD21o+IhRGxKiJWOWKdJPWserplhgJzgFHAYcC+wHvbWbTdgaQz85rMbMzMxmHDhnW3DElSO+rpljkZeCIzN2fmH4HbgHcBB1bdNAANwNN11ihJ6qJ6wv1JYHJE7BMRAcwA1gD3AWdWy8wH7qivRElSV9XT5/4QtROnPwN+Ub3WNcCFwN9FxDrgYOD6HqhTktQFdX0TU2ZeBly2Q/PjwKR6XleSVB/vUJWkAhnuklQgw12SCmS4S1KBDHdJKpDhLkkFMtwlqUCGuyQVyHCXpAIZ7pJUIMNdkgpkuEtSgQx3SSqQ4S5JBTLcJalAhrskFchwl6QCGe6SVCDDXZIKZLhLUoEMd0kqkOEuSQUy3CWpQIa7JBXIcJekAhnuklQgw12SCmS4S1KBDHdJKpDhLkkFMtwlqUCGuyQVqK5wj4gDI2JZRPwqItZGxJSIOCgiVkREc/U4tKeKlSR1Tr1H7t8A/ndmvhOYAKwFFgH3ZuZo4N5qXpK0G3U73CPiAGAacD1AZr6amb8H5gBLq8WWAmfUW6QkqWvqOXI/EtgM/I+I+LeIuC4i9gUOzcwNANXj8B6oU5LUBfWE+yDgOODbmXks8Ae60AUTEQsjYlVErNq8eXMdZUiSdlRPuLcALZn5UDW/jFrYb4yIEQDV46b2Vs7MazKzMTMbhw0bVkcZkqQddTvcM/MZ4HcR8Y6qaQawBrgTmF+1zQfuqKtCSVKXDapz/c8AN0bEnsDjwMep/cH4QUQsAJ4EzqpzG5KkLqor3DPzYaCxnadm1PO6kqT6eIeqJBXIcJekAhnuklQgw12SCmS4S1KBDHdJKpDhLkkFMtwlqUCGuyQVyHCXpAIZ7pJUIMNdkgpkuEtSgQx3SSqQ4S5JBTLcJalAhrskFchwl6QCGe6SVCDDXZIKZLhLUoEMd0kqkOEuSQUy3CWpQIa7JBXIcJekAhnuklQgw12SCmS4S1KBDHdJKpDhLkkFMtwlqUCD+roAdd3IRT/q9rrrF5/Wg5VI6q/qPnKPiD0i4t8i4q5qflREPBQRzRFxc0TsWX+ZkqSu6Ilumc8Ca9vMXwF8LTNHA88DC3pgG5KkLqgr3COiATgNuK6aD+A9wLJqkaXAGfVsQ5LUdfUeuX8d+AdgWzV/MPD7zHytmm8BDm9vxYhYGBGrImLV5s2b6yxDktRWt8M9It4HbMrM1W2b21k021s/M6/JzMbMbBw2bFh3y5AktaOeq2VOAk6PiFOBIcAB1I7kD4yIQdXRewPwdP1lSpK6ottH7pl5UWY2ZOZI4Bzg/2bmucB9wJnVYvOBO+quUpLUJb1xE9OFwN9FxDpqffDX98I2JElvokduYsrMlcDKavpxYFJPvK4kqXscfkCSCmS4S1KBDHdJKpDhLkkFclTIt5i2I0o+s+eWndo64qiS0sDgkbskFchwl6QCGe6SVCD73PtIPd+mJEkd8chdkgpkuEtSgeyWUZf45dzSwOCRuyQVyHCXpAIZ7pJUIMNdkgpkuEtSgQx3SSqQ4S5JBTLcJalAhrskFchwl6QCGe6SVCDDXZIKZLhLUoEMd0kqkOEuSQUy3CWpQIa7JBXIcJekAhnuklQgw12SCtTtcI+It0XEfRGxNiJ+GRGfrdoPiogVEdFcPQ7tuXIlSZ1Rz5H7a8B/zcwxwGTgryNiLLAIuDczRwP3VvOSpN2o2+GemRsy82fV9AvAWuBwYA6wtFpsKXBGvUVKkrqmR/rcI2IkcCzwEHBoZm6A2h8AYPgu1lkYEasiYtXmzZt7ogxJUqXucI+I/YBbgb/NzP/o7HqZeU1mNmZm47Bhw+otQ5LUxqB6Vo6IwdSC/cbMvK1q3hgRIzJzQ0SMADbVW6TKMHLRjzq13Pcf3wLAOW2WX7/4tF6pSSpVPVfLBHA9sDYzv9rmqTuB+dX0fOCO7pcnSeqOeo7cTwL+EvhFRDxctV0MLAZ+EBELgCeBs+orUZLUVd0O98x8AIhdPD2ju68rSaqfd6hKUoEMd0kqkOEuSQUy3CWpQIa7JBXIcJekAhnuklQgw12SCmS4S1KBDHdJKpDhLkkFMtwlqUCGuyQVyHCXpAIZ7pJUoLq+Zu+trrNfG6f61bOv/Yo+vRV55C5JBTLcJalAhrskFegt3+duv7mkEnnkLkkFMtwlqUCGuyQVyHCXpAIZ7pJUIMNdkgpkuEtSgQx3SSqQ4S5JBTLcJalAhrskFchwl6QCDfiBwxz4Sx3xiz7Um+rNoN56j/XKkXtEzI6IX0fEuohY1BvbkCTtWo8fuUfEHsC3gJlAC/CvEXFnZq7p6W1Jva0vj8r66hPHQP2d61HiJ7TeOHKfBKzLzMcz81Xg+8CcXtiOJGkXIjN79gUjzgRmZ+Ynq/m/BE7MzAt2WG4hsLCafQfw6x4tpOcdAjzb10V000Ct3bp3v4Fa+0CtG+qr/e2ZOay9J3rjhGq007bTX5DMvAa4phe23ysiYlVmNvZ1Hd0xUGu37t1voNY+UOuG3qu9N7plWoC3tZlvAJ7uhe1IknahN8L9X4HRETEqIvYEzgHu7IXtSJJ2oce7ZTLztYi4APg/wB7ADZn5y57eTh8YMF1I7RiotVv37jdQax+odUMv1d7jJ1QlSX3P4QckqUCGuyQVyHBvR0RMiIj/FxG/iIh/jogD2jx3UTWswq8j4pQ27X0+5EJETIyIn0bEwxGxKiImVe0REVdXtf08Io5rs878iGiufub3Rd1VHTdXdT8cEesj4uE2z/XbfV7V8Zmqjl9GxH9r095v646IL0TEU232+akDoe62IuLvIyIj4pBqvl+/zyPiy1VdD0fE8og4rFfrzkx/dvihdsXPu6vpTwBfrqbHAo8AewGjgMeonTTeo5o+EtizWmZsH9S9HHhvNX0qsLLN9P+idg/CZOChqv0g4PHqcWg1PbQf7P//Dlw6QPb5fwHuAfaq5ocPkLq/APx9O+39uu42db6N2kUbvwUOGQjvc+CANtN/A/xTb9btkXv73gHcX02vAD5YTc8Bvp+Zr2TmE8A6asMt9JchFxLY/injP/H6/QVzgO9kzU+BAyNiBHAKsCIzn8vM56n9rrN3d9FtRUQAZwM3VU39fZ//FbA4M18ByMxNA6TuXRkodX8N+AfeeINkv36fZ+Z/tJndl9dr75W6Dff2PQqcXk2fxes3ZR0O/K7Nci1V267ad7e/Ba6MiN8BVwEXVe39ve62/gLYmJnN1Xx/r/0o4C8i4qGI+JeIOKFq7+91A1xQdQPcEBFDq7Z+X3dEnA48lZmP7PDUQKj9H6v/n+cCl1bNvVL3gB/Pvbsi4h7gP7fz1CXUumKujohLqd2A9er21dpZPmn/j2SvXGPaQd0zgM9l5q0RcTZwPXAyu667U0NF9JQ3qz0z76im5/H6UTv0/30+iNpH5snACcAPIuJI+n/d3wa+XG37y9S6wj5BP6gbOqz9YmBWe6u107Zb3+cdvccz8xLgkoi4CLgAuOxN6qur7rdsuGfmyR0sMgsgIo4Cto8H+mZDK+yWIRferO6I+A7w2Wr2FuC6anpXdbcA03doX9lDpe6ko30eEYOADwDHt2nu7/v8r4DbstZJ2hQR26gNBNWv624rIq4F7qpm+7xu2HXtEXEMtXMBj9R68GgAflZdPNDn7/PO7nPgfwI/ohbuvVN3X5wM6e8/vH5S7M+A7wCfqOaP5o0nmx6ndqJpUDU9itdPNh3dB3WvBaZX0zOA1dX0abzxhE1Tvn7C5glqR55Dq+mD+nC/zwb+ZYe2/r7Pzwe+VE0fRe1jdAyAuke0mf4ctX72fr+/2/k91vP6CdV+/T4HRreZ/gywrDfr7tN/mP76Q+3o9zfVz2KqO3mr5y6hdtXAr6muTKnaT62Wf4zaR7C+qHsqsLr6j/cQcHzVHtS+QOUx4BdAY5t1PkHtpNk64ON9vN+XAOe3096f9/mewPeonaf5GfCeAVL3d6v3ws+pdT22Dft+W3c7v0fbcO/X73Pg1up98nPgn4HDe7Nuhx+QpAJ5tYwkFchwl6QCGe6SVCDDXZIKZLhLUoEMd0kqkOEuSQX6/0ZNLklZ0+oRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's see the initial reward distribution\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sample_rewards = [generate_session(policy, t_max=1000)[-1] for _ in range(200)]\n",
    "\n",
    "plt.hist(sample_rewards, bins=20)\n",
    "plt.vlines([np.percentile(sample_rewards, 50)], [0], [100], label=\"50'th percentile\", color='green')\n",
    "plt.vlines([np.percentile(sample_rewards, 90)], [0], [100], label=\"90'th percentile\", color='red')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crossentropy method steps (2pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import compress\n",
    "\n",
    "def select_elites(states_batch, actions_batch, rewards_batch, percentile=50):\n",
    "    \"\"\"\n",
    "    Select states and actions from games that have rewards >= percentile\n",
    "    :param states_batch: list of lists of states, states_batch[session_i][t]\n",
    "    :param actions_batch: list of lists of actions, actions_batch[session_i][t]\n",
    "    :param rewards_batch: list of rewards, rewards_batch[session_i]\n",
    "\n",
    "    :returns: elite_states,elite_actions, both 1D lists of states and respective actions from elite sessions\n",
    "\n",
    "    Please return elite states and actions in their original order \n",
    "    [i.e. sorted by session number and timestep within session]\n",
    "\n",
    "    If you are confused, see examples below. Please don't assume that states are integers\n",
    "    (they will become different later).\n",
    "    \"\"\"\n",
    "    states_batch, actions_batch, rewards_batch = map(np.array, [states_batch, actions_batch, rewards_batch])\n",
    "    reward_threshold = np.percentile(rewards_batch, percentile)\n",
    "    \n",
    "    elite_sessions_mask = rewards_batch >= reward_threshold\n",
    "\n",
    "    elite_states = np.hstack(states_batch[elite_sessions_mask])\n",
    "    elite_actions = np.hstack(actions_batch[elite_sessions_mask])\n",
    "\n",
    "    return elite_states, elite_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok!\n"
     ]
    }
   ],
   "source": [
    "states_batch = [\n",
    "    [1, 2, 3],     # game1\n",
    "    [4, 2, 0, 2],  # game2\n",
    "    [3, 1],        # game3\n",
    "]\n",
    "\n",
    "actions_batch = [\n",
    "    [0, 2, 4],     # game1\n",
    "    [3, 2, 0, 1],  # game2\n",
    "    [3, 3],        # game3\n",
    "]\n",
    "rewards_batch = [\n",
    "    3,  # game1\n",
    "    4,  # game2\n",
    "    5,  # game3\n",
    "]\n",
    "\n",
    "test_result_0 = select_elites(\n",
    "    states_batch, actions_batch, rewards_batch, percentile=0)\n",
    "test_result_40 = select_elites(\n",
    "    states_batch, actions_batch, rewards_batch, percentile=30)\n",
    "test_result_90 = select_elites(\n",
    "    states_batch, actions_batch, rewards_batch, percentile=90)\n",
    "test_result_100 = select_elites(\n",
    "    states_batch, actions_batch, rewards_batch, percentile=100)\n",
    "\n",
    "assert np.all(test_result_0[0] == [1, 2, 3, 4, 2, 0, 2, 3, 1])  \\\n",
    "    and np.all(test_result_0[1] == [0, 2, 4, 3, 2, 0, 1, 3, 3]),\\\n",
    "    \"For percentile 0 you should return all states and actions in chronological order\"\n",
    "assert np.all(test_result_40[0] == [4, 2, 0, 2, 3, 1]) and \\\n",
    "    np.all(test_result_40[1] == [3, 2, 0, 1, 3, 3]),\\\n",
    "    \"For percentile 30 you should only select states/actions from two first\"\n",
    "assert np.all(test_result_90[0] == [3, 1]) and \\\n",
    "    np.all(test_result_90[1] == [3, 3]),\\\n",
    "    \"For percentile 90 you should only select states/actions from one game\"\n",
    "assert np.all(test_result_100[0] == [3, 1]) and\\\n",
    "    np.all(test_result_100[1] == [3, 3]),\\\n",
    "    \"Please make sure you use >=, not >. Also double-check how you compute percentile.\"\n",
    "print(\"Ok!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(elite_states, elite_actions):\n",
    "    \"\"\"\n",
    "    Given old policy and a list of elite states/actions from select_elites,\n",
    "    return new updated policy where each action probability is proportional to\n",
    "\n",
    "    policy[s_i,a_i] ~ #[occurences of si and ai in elite states/actions]\n",
    "\n",
    "    Don't forget to normalize policy to get valid probabilities and handle 0/0 case.\n",
    "    In case you never visited a state, set probabilities for all actions to 1./n_actions\n",
    "\n",
    "    :param elite_states: 1D list of states from elite sessions\n",
    "    :param elite_actions: 1D list of actions from elite sessions\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    new_policy = np.zeros([n_states, n_actions])\n",
    "    for i, j in zip(elite_states, elite_actions):\n",
    "        new_policy[i, j] += 1.\n",
    "    \n",
    "    sums = new_policy.sum(axis=1)\n",
    "    not_visited = sums < .1\n",
    "    visited = ~not_visited\n",
    "    \n",
    "    new_policy[not_visited] = 1. / n_actions\n",
    "    new_policy[visited] /= sums[visited, np.newaxis]\n",
    "\n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(elite_states, elite_actions):\n",
    "    \"\"\"\n",
    "    Given old policy and a list of elite states/actions from select_elites,\n",
    "    return new updated policy where each action probability is proportional to\n",
    "\n",
    "    policy[s_i,a_i] ~ #[occurences of si and ai in elite states/actions]\n",
    "\n",
    "    Don't forget to normalize policy to get valid probabilities and handle 0/0 case.\n",
    "    In case you never visited a state, set probabilities for all actions to 1./n_actions\n",
    "\n",
    "    :param elite_states: 1D list of states from elite sessions\n",
    "    :param elite_actions: 1D list of actions from elite sessions\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    elite_states, elite_actions = map(np.array, (elite_states, elite_actions))\n",
    "    new_policy = 1. * np.bincount(elite_states * n_actions + elite_actions, minlength=n_states*n_actions).reshape((n_states, n_actions))\n",
    "    \n",
    "    sums = np.bincount(elite_states, minlength=n_states)\n",
    "    not_visited = sums < .1\n",
    "    visited = ~not_visited\n",
    "    \n",
    "    new_policy[not_visited] = 1. / n_actions\n",
    "    new_policy[visited] /= sums[visited, np.newaxis]\n",
    "\n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "\n",
    "def update_policy(elite_states, elite_actions):\n",
    "    \"\"\"\n",
    "    Given old policy and a list of elite states/actions from select_elites,\n",
    "    return new updated policy where each action probability is proportional to\n",
    "\n",
    "    policy[s_i,a_i] ~ #[occurences of si and ai in elite states/actions]\n",
    "\n",
    "    Don't forget to normalize policy to get valid probabilities and handle 0/0 case.\n",
    "    In case you never visited a state, set probabilities for all actions to 1./n_actions\n",
    "\n",
    "    :param elite_states: 1D list of states from elite sessions\n",
    "    :param elite_actions: 1D list of actions from elite sessions\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    data = np.ones(len(elite_states))\n",
    "    ijv = (data, (elite_states, elite_actions))\n",
    "    new_policy = coo_matrix(ijv, shape=(n_states, n_actions), dtype=float).toarray()\n",
    "    \n",
    "    sums = new_policy.sum(axis=1)\n",
    "    not_visited = sums < .1\n",
    "    visited = ~not_visited\n",
    "    \n",
    "#     new_policy = new_policy.toarray()\n",
    "    new_policy[not_visited] = 1. / n_actions\n",
    "    new_policy[visited] /= sums[visited, np.newaxis]\n",
    "\n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok!\n"
     ]
    }
   ],
   "source": [
    "elite_states = [1, 2, 3, 4, 2, 0, 2, 3, 1]\n",
    "elite_actions = [0, 2, 4, 3, 2, 0, 1, 3, 3]\n",
    "\n",
    "new_policy = update_policy(elite_states, elite_actions)\n",
    "\n",
    "assert np.isfinite(new_policy).all(\n",
    "), \"Your new policy contains NaNs or +-inf. Make sure you don't divide by zero.\"\n",
    "assert np.all(\n",
    "    new_policy >= 0), \"Your new policy can't have negative action probabilities\"\n",
    "assert np.allclose(new_policy.sum(\n",
    "    axis=-1), 1), \"Your new policy should be a valid probability distribution over actions\"\n",
    "reference_answer = np.array([\n",
    "    [1.,  0.,  0.,  0.,  0.],\n",
    "    [0.5,  0.,  0.,  0.5,  0.],\n",
    "    [0.,  0.33333333,  0.66666667,  0.,  0.],\n",
    "    [0.,  0.,  0.,  0.5,  0.5]])\n",
    "assert np.allclose(new_policy[:4, :5], reference_answer)\n",
    "print(\"Ok!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "Generate sessions, select N best and fit to those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def show_progress(rewards_batch, log, percentile, reward_range=[-990, +30]):\n",
    "    \"\"\"\n",
    "    A convenience function that displays training progress. \n",
    "    No cool math here, just charts.\n",
    "    \"\"\"\n",
    "   \n",
    "    mean_reward = np.mean(rewards_batch)\n",
    "    threshold = np.percentile(rewards_batch, percentile)\n",
    "    log.append([mean_reward, threshold])\n",
    "\n",
    "    if len(log) % 5 != 1:\n",
    "        return\n",
    "\n",
    "    clear_output(True)\n",
    "    print(\"mean reward = %.3f, threshold=%.3f\" % (mean_reward, threshold))\n",
    "    plt.figure(figsize=[8, 4])\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(list(zip(*log))[0], label='Mean rewards')\n",
    "    plt.plot(list(zip(*log))[1], label='Reward thresholds')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(rewards_batch, range=reward_range)\n",
    "    plt.vlines([np.percentile(rewards_batch, percentile)],\n",
    "               [0], [100], label=\"percentile\", color='red')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 272 µs, sys: 2 µs, total: 274 µs\n",
      "Wall time: 278 µs\n",
      "CPU times: user 262 µs, sys: 0 ns, total: 262 µs\n",
      "Wall time: 266 µs\n",
      "CPU times: user 318 µs, sys: 0 ns, total: 318 µs\n",
      "Wall time: 322 µs\n",
      "CPU times: user 262 µs, sys: 0 ns, total: 262 µs\n",
      "Wall time: 265 µs\n",
      "CPU times: user 262 µs, sys: 2 µs, total: 264 µs\n",
      "Wall time: 268 µs\n"
     ]
    }
   ],
   "source": [
    "# reset policy just in case\n",
    "policy = np.ones([n_states, n_actions]) / n_actions\n",
    "\n",
    "n_sessions = 250  # sample this many sessions\n",
    "percentile = 50  # take this percent of session with highest rewards\n",
    "learning_rate = 0.5  # add this thing to all counts for stability\n",
    "\n",
    "log = []\n",
    "\n",
    "for i in range(5):\n",
    "    sessions = [generate_session(policy, t_max=500) for _ in range(n_sessions)]\n",
    "\n",
    "    states_batch, actions_batch, rewards_batch = zip(*sessions)\n",
    "    elite_states, elite_actions = select_elites(states_batch, actions_batch, rewards_batch, percentile)\n",
    "\n",
    "    %time new_policy = update_policy(elite_states, elite_actions)\n",
    "\n",
    "    policy = learning_rate*new_policy + (1-learning_rate)*policy\n",
    "\n",
    "    # display results on chart\n",
    "#     show_progress(rewards_batch, log, percentile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### You're not done yet!\n",
    "\n",
    "Go to [`./deep_crossentropy_method.ipynb`](./deep_crossentropy_method.ipynb) for a more serious task"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
